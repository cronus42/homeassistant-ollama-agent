# Home Assistant Ollama Conversation Integration

[![hacs_badge](https://img.shields.io/badge/HACS-Custom-orange.svg)](https://github.com/custom-components/hacs)
[![GitHub Release](https://img.shields.io/github/release/yourusername/homeassistant-ollama-agent.svg)](https://github.com/yourusername/homeassistant-ollama-agent/releases)
[![License](https://img.shields.io/github/license/yourusername/homeassistant-ollama-agent.svg)](LICENSE)

A Home Assistant custom integration that provides conversation agent capabilities using Ollama with full tool/function calling support for device control.

## âœ¨ Features

- ğŸ¤– **Local AI Processing**: Run language models locally via Ollama
- ğŸ”§ **Tool/Function Calling**: Native support for Home Assistant device control
- ğŸ’¡ **Smart Home Control**: Control lights, climate, and other devices through natural language
- ğŸ”„ **Conversation History**: Maintains context across multiple turns
- âš™ï¸ **Configurable Parameters**: Adjust temperature, context window, and other model settings
- ğŸ¯ **Optimized for Function-Capable Models**: Designed to work with tool-capable models like Home-FunctionGemma-270m

## ğŸš€ Quick Start

### Prerequisites

1. **Ollama Server**: Running locally or on your network
2. **Compatible Model**: Download a tool-capable model:
   ```bash
   ollama pull home-functiongemma-270m
   ```

### Installation via HACS (Recommended)

1. Open HACS in Home Assistant
2. Go to "Integrations"
3. Click the three dots menu â†’ "Custom repositories"
4. Add this repository URL
5. Search for "Ollama Conversation" and install
6. Restart Home Assistant

### Manual Installation

1. Copy the `custom_components/ollama_conversation` folder to your Home Assistant `config/custom_components/` directory
2. Restart Home Assistant

### Configuration

1. Go to **Settings** â†’ **Devices & Services**
2. Click **"+ Add Integration"**
3. Search for **"Ollama Conversation"**
4. Enter your Ollama server URL (default: `http://sanctuarymoon.local:11434`)
5. Select your model from the dropdown
6. Configure optional parameters (temperature, context window, etc.)

## ğŸ“– Documentation

See the [detailed README](custom_components/ollama_conversation/README.md) for:
- Usage examples
- Supported device types
- Architecture details
- Troubleshooting guide
- API documentation

## ğŸ†š Comparison with Alternatives

### vs. home-llm Integration

| Feature | home-llm | ollama_conversation |
|---------|----------|---------------------|
| Ollama Support | âœ… Basic | âœ… Full API |
| Tool Calling | âŒ Limited | âœ… Native |
| UI Configuration | âŒ YAML only | âœ… Config Flow |
| Model Switching | âŒ Manual | âœ… Dropdown |
| Error Handling | âš ï¸ Basic | âœ… Comprehensive |

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file for details

## ğŸ™ Credits

- Built by **goose** (Block's AI agent)
- Inspired by Home Assistant's `bedrock` integration
- Compatible with **Home-FunctionGemma-270m** model

## ğŸ”’ Privacy

This integration processes everything locally. Your conversations never leave your network when using Ollama locally.

## ğŸ’¬ Support

- ğŸ“– [Documentation](custom_components/ollama_conversation/README.md)
- ğŸ› [Issues](https://github.com/yourusername/homeassistant-ollama-agent/issues)
- ğŸ’¬ [Home Assistant Community](https://community.home-assistant.io)
